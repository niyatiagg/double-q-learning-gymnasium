{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjTLNrQY5ROoeYfIsmbHOB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "environment = \"C\"\n",
        "\n",
        "from typing import Any\n",
        "import random\n",
        "import gymnasium as gym\n",
        "\n",
        "def argmax_action(d: dict[Any,float]) -> Any:\n",
        "    \"\"\"return a key of the maximum value in a given dictionary\n",
        "\n",
        "    Args:\n",
        "        d (dict[Any,float]): dictionary\n",
        "\n",
        "    Returns:\n",
        "        Any: a key\n",
        "    \"\"\"\n",
        "    if not d:\n",
        "        return None\n",
        "    max_value = float('-inf')\n",
        "    max_key = None\n",
        "    for k,v in d.items():\n",
        "        if v > max_value:\n",
        "            max_value = v\n",
        "            max_key = k\n",
        "\n",
        "    return k\n",
        "\n",
        "class ValueRLAgent():\n",
        "    def __init__(self, env: gym.Env, gamma : float = 0.98, eps: float = 0.2, alpha: float = 0.02, total_epi: int = 5_000) -> None:\n",
        "        \"\"\"initialize agent parameters\n",
        "        This class will be a parent class and not be called directly.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): gym environment\n",
        "            gamma (float, optional): a discount factor. Defaults to 0.98.\n",
        "            eps (float, optional): the epsilon value. Defaults to 0.2. Note: this pa uses a simple eps-greedy not decaying eps-greedy.\n",
        "            alpha (float, optional): a learning rate. Defaults to 0.02.\n",
        "            total_epi (int, optional): total number of episodes an agent should learn. Defaults to 5_000.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.q = self.init_qtable(env.observation_space.n, env.action_space.n)\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.total_epi = total_epi\n",
        "\n",
        "    def init_qtable(self, n_states: int, n_actions: int, init_val: float = 0.0) -> dict[int,dict[int,float]]:\n",
        "        \"\"\"initialize the q table (dictionary indexed by s, a) with a given init_value\n",
        "\n",
        "        Args:\n",
        "            n_states (int, optional): the number of states. Defaults to int.\n",
        "            n_actions (int, optional): the number of actions. Defaults to int.\n",
        "            init_val (float, optional): all q(s,a) should be set to this value. Defaults to 0.0.\n",
        "\n",
        "        Returns:\n",
        "            dict[int,dict[int,float]]: q table (q[s][a] -> q-value)\n",
        "        \"\"\"\n",
        "        q = dict()\n",
        "        for s in range(n_states):\n",
        "            q[s] = dict()\n",
        "            for a in range(n_actions):\n",
        "                q[s][a] = init_val\n",
        "\n",
        "        return q\n",
        "\n",
        "    def eps_greedy(self, state: int, exploration: bool = True) -> int:\n",
        "        \"\"\"epsilon greedy algorithm to return an action\n",
        "\n",
        "        Args:\n",
        "            state (int): state\n",
        "            exploration (bool, optional): explore based on the epsilon value if True; take the greedy action by the current self.q if False. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            int: action\n",
        "        \"\"\"\n",
        "        greedy_action = argmax_action(self.q[state])\n",
        "        if exploration:\n",
        "            per_action_eps = self.eps / self.env.action_space.n\n",
        "            prob_for_each_action = [per_action_eps] * self.env.action_space.n\n",
        "            prob_for_each_action[greedy_action] = 1 - self.eps + per_action_eps\n",
        "            random_action = random.choices(range(self.env.action_space.n), weights=prob_for_each_action)[0]\n",
        "            return random_action\n",
        "        else:\n",
        "            return greedy_action\n",
        "\n",
        "    def choose_action(self, ss: int) -> int:\n",
        "        \"\"\"a helper function to specify a exploration policy\n",
        "        If you want to use the eps_greedy, call the eps_greedy function in this function and return the action.\n",
        "\n",
        "        Args:\n",
        "            ss (int): state\n",
        "\n",
        "        Returns:\n",
        "            int: action\n",
        "        \"\"\"\n",
        "        return self.eps_greedy(ss)\n",
        "\n",
        "    def best_run(self, max_steps: int = 100) -> tuple[list[tuple[int,int,float]], bool]:\n",
        "        \"\"\"After the learning, an optimal episode (based on the latest self.q) needs to be generated for evaluation. From the initial state, always take the greedily best action until it reaches a goal.\n",
        "\n",
        "        Args:\n",
        "            max_steps (int, optional): Terminate the episode generation if the agent cannot reach the goal after max_steps. One step is (s,a,r) Defaults to 100.\n",
        "\n",
        "        Returns:\n",
        "            tuple[\n",
        "                list[tuple[int,int,float]],: An episode [(s1,a1,r1), (s2,a2,r2), ...]\n",
        "                bool: done - True if the episode reaches a goal, False if it hits max_steps.\n",
        "            ]\n",
        "        \"\"\"\n",
        "        episode = list()\n",
        "        done = False\n",
        "        current_state = self.env.reset(seed=42)[0]\n",
        "        for _ in range(max_steps):\n",
        "            action = argmax_action(self.q[current_state])\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            episode.append((current_state, action, reward))\n",
        "            if terminated or truncated:\n",
        "                done = True\n",
        "                break\n",
        "            current_state = next_state\n",
        "\n",
        "        print(f\"Episode length: {len(episode)}, Total Return: {self.calc_return(episode, done)}\")\n",
        "        return (episode, done)\n",
        "\n",
        "    def calc_return(self, episode: list[tuple[int,int,float]], done=False) -> float:\n",
        "        \"\"\"Given an episode, calculate the return value. An episode is in this format: [(s1,a1,r1), (s2,a2,r2), ...].\n",
        "\n",
        "        Args:\n",
        "            episode (list[tuple[int,int,float]]): An episode [(s1,a1,r1), (s2,a2,r2), ...]\n",
        "            done (bool, optional): True if the episode reaches a goal, False if it does not. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            float: the return value. None if done is False.\n",
        "        \"\"\"\n",
        "\n",
        "        if not done:\n",
        "            return None\n",
        "\n",
        "        return_value = 0\n",
        "        num = 0\n",
        "        for obs in episode:\n",
        "            return_value += obs[2] * self.gamma ** num\n",
        "            num += 1\n",
        "\n",
        "        return return_value\n",
        "\n",
        "class DoubleQLAgent(ValueRLAgent):\n",
        "    def __init__(self, env, gamma = 0.98, eps = 0.2, alpha = 0.02, total_epi = 5000):\n",
        "        super().__init__(env, gamma, eps, alpha, total_epi)\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Double Q-Learning algorithm\n",
        "        Update the Q table (self.q) for self.total_epi number of episodes.\n",
        "\n",
        "        The results should be reflected to its q table.\n",
        "        \"\"\"\n",
        "        q1_table = self.init_qtable(self.env.observation_space.n, self.env.action_space.n)\n",
        "        q2_table = self.init_qtable(self.env.observation_space.n, self.env.action_space.n)\n",
        "        for _ in range(self.total_epi):\n",
        "            current_state = self.env.reset(seed=42)[0]\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(current_state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                if random.random() < 0.5:\n",
        "                    q1_table[current_state][action] = q1_table[current_state][action] + self.alpha * (reward + self.gamma * q2_table[next_state][argmax_action(q1_table[next_state])] - q1_table[current_state][action])\n",
        "                else:\n",
        "                    q2_table[current_state][action] = q2_table[current_state][action] + self.alpha * (reward + self.gamma * q1_table[next_state][argmax_action(q2_table[next_state])] - q2_table[current_state][action])\n",
        "\n",
        "                self.q[current_state][action] = (q1_table[current_state][action] + q2_table[current_state][action]) / 2\n",
        "                current_state = next_state\n",
        "                if terminated or truncated:\n",
        "                    done = True\n",
        "                    break\n",
        "\n",
        "# TODO : Add the plot thing and record the best epi; to be added in PDF file\n",
        "\n",
        "my_agent = DoubleQLAgent(env=gym.make(\"CliffWalking-v1\"), render_mode=\"human\")\n",
        "my_agent.learn()\n",
        "my_agent.best_run()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lYbs0F_LFCdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}